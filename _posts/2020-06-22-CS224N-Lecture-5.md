---
layout: article
mathjax: true
mermaid: true
title: "CS224n Lecture 5"
author: "Thinh Ngo"
categories: journal
tags: [CS224N, NLP, Machine learning, Dependency parser, Transition-based dependency parsing]
---

## Dependency grammar and dependency structure

Dependency structure of sentences shows which words depend on (modify or are arguments of) which other words. These binary asymmetric relations between the words are called dependencies and are depicted as arrows going from the **head** (or governor, superior, regent) to the **dependent** (or modifier, inferior, subordinate).

An example of dependency tree for the sentence "Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas".

```mermaid
classDiagram
	on <-- ports: case
	and <-- ports: cc
	immigration <-- ports: conj
	ports <-- Bills: nmod
	of <-- Kansas: case
	Kansas <-- Republican: nmod
	Republican <-- Brownback: appros
	Senator <-- Brownback: flat
	by <-- Brownback: case
	Brownback <-- submitted: obl
	were <-- submitted: aux
	Bills <-- submitted: nsubj:pass
	
```



### Dependency parsing

Dependency parsing is the task of analyzing the syntactic dependency structure of a given input sentence $S.$ The output of a dependency parser is a dependency tree where the words of the input sentence are connected by typed dependency relations. There are two subproblems in dependency parsing:

- *Learning*: Given a training set of sentences annotated with dependency graphs, induce a parsing model that can be used to parse new sentences.
- *Parsing*: Given a parsing model and a sentence, derive the dependency graph for it.

### Transition-based dependency parsing

Transition-based dependency parsing relies on a state machine which defines the possible transitions to create the mapping from the input sentence to the dependency tree. The *learning problem* is to induce a model which can predict the next transition in the state machine based on the transition history. The *parsing problem* is to construct an optimal sequence of transitions for the input sentence, given the previous induced model.

Most transition-based systems do not make use of a formal grammar.

### Greedy deterministic transition-based parsing

This model induces a sequence of transitions from some *initial* state to one of several *terminal* states.

**States**:

A state can be described by a triple of:

1. a stack
2. a buffer
3. a set of dependency arcs

For any sentence, there are:

1. an initial state (only the ROOT on the stack, others are in the buffer and no actions have been chosen yet)
2. a terminal state (nothing in the buffer)

**Transitions**:

1. SHIFT: Remove the first word in the buffer and push it on top of the stack.
2. LEFT-ARC: Add a dependency arc $w_j \rightarrow w_i$ in which, $w_i$ is the word on top of the stack and $w_j$ is the second to the top.
3. RIGHT-ARC: Add a dependency arc $w_i \rightarrow w_j$.

### Neural dependency parsing

How do we predict the next action/transition given the current state/configuration?

Then it comes to machine learning and of course, the deep neural network in particular.