---
layout: article
mathjax: true
title: "CS224n Lecture 2"
author: "Thinh Ngo"
tags: [CS224N, NLP, Machine Learning, Word2vec, Negative Sampling, Co-occurrence matrix, GloVe, Intrinsic, Extrinsic]
image: glove.jpg
---
## 1. Continue with Word2vec

Remind in the last lecture: the probability of the existence a context word $o$ given a center word $c$ is proposed to be calculated by:

$$P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}$$

However, a problem is raised as the computing complexity of this soft-max function is huge. We have to compute the similarity between all words in the corpus and the center word. Thus, the **negative sampling** is then can be applied as an alternative.

### Negative sampling

We reformulate the objective function (of the maximization problem):

$$J_t(\theta) = \log \sigma(u_o^T v_c) + \sum_{k=1}^K\mathbf{E}_{j \sim P(w)}[\log\sigma(-u_j^T v_c)]$$

We can interpret the maximisation problem as trying to maximize the probability of existence of a word $o$ that **did exist** in a context window of the center word $c$ while **penalizing** the existence of $k$ **sampling words**.

## 2. Co-occurrence matrix and Dimensionality reduction

We can also capture word vectors just by counting. Co-occurrence matrix $M$ appears to be a symmetric matrix of size $n\times n$ where $n$ is the number of vocabulary in the whole corpus. When observing a word $j$ appearing in the context window of a center word $i$, we increment $M_{ij}$ by one.

However, the problem raised by co-occurrence matrix is its size increasing with corpus, high dimensionality requiring much storage and its sparsity.

One solution introduced in the lecture is **Singular Value Decomposition** (**SVD**). The core idea of the method is by finding another matrix having only $k$ singular values and approximating to the co-occurrence matrix. More details should be found in the [lecture of Prof. Gilbert Strang](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/index.htm).


## 3. GloVe: Combining the best of both worlds

The principal idea of GloVe is to encode meaning components by the ratio of co-occurrence probabilities.

![GloVe idea](https://raw.githubusercontent.com/thinhngo-x/blog/gh-pages/assets/img/GloVe.png)



## 4. Intrinsic and Extrinsic evaluation
