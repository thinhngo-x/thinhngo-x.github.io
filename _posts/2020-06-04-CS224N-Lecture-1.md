---
layout: article
title: "CS224n Lecture 1"
author: Thinh Ngo
tags: [CS224N, NLP, Machine Learning, Word2vec]
image: Word2vec.png
mathjax: true
---

## 1. Intro
Wordnet is  thesaurus containing lists of **synonym sets** and **hypernyms**. However, it cannot represent the meaning of a word in a particular context.

Words can be represented by one-hot vectors which contain binary values *i.e.* 0 and 1. The dimension of vector equals to the number of words in vocabulary. This representation allows to encode the context of words but lacks of notion **similarity** as two word-vectors could always be orthogonal to each other regardless of their meanings.

>The **problem**: Trying to encode the similarity of words in numerical vectors

## 2. Word2vec
A **center** word *c* and its **context** words *o* are those appearing nearby in a text.

Word vectors = Word embeddings = Word representations or vice versa.

Idea:
>Use the **similarity of the word vectors** of *c* and *o* to calculate probability of context word *o* given the center word *c*. Keep adjusting this vector to maximize the probability.

### 2.1. Parametric model
Optimization problem with a context window size $t$, a vocabulary of $T$ words:

$$\displaystyle \text{Given the likelihood function} \ L(\theta) = \displaystyle \prod_{i=1}^T \prod_{\substack{-t \leq j \leq t \\ j \neq 0}} P(w_{i+j}|w_i, \theta)$$

The objective is to maximize this likelihood function. Take average of $\log$ of the function, we have a new objective function of the minimization problem instead:

$$
J(\theta) = \displaystyle - \frac{1}{T} \sum_{i=1}^T \sum_{\substack{-t \leq j \leq t \\ j \neq 0}}\log P(w_{i+j}|w_i, \theta)
$$

However it remains to estimate the probability $P(w_{i+j}|w_i, \theta)$. Thus, it is estimated by two principle rules:
* The **similarity** can be indicated by a dot product of word vectors.
* The soft-max function is a good way to normalize and to produce a reasonable probability. **Max** indicates that we focus on the maximum probability and even amplify it with the exponential functio. **Soft** means that we still consider the smaller probabilities.

By that, **Word2vec** proposed to estimate the probability as:

$$P(w|c) = \frac{\exp(u_w^T v_c)}{\sum_{i=1}^T{\exp(u_i v_c})}$$

In that case, the parameter $\theta$ of the statistic model is defined by:

$$\theta = \begin{bmatrix}u_1 \\ u_2 \\ \vdots \\ u_T \\ v_1 \\ v_2 \\ \vdots \\ v_T\end{bmatrix}$$

It is essential to note that one word requires two representative vectors, one for context representation and one for center.

### 2.2. Optimization algorithm

Gradient descent is commonly-used in Deep Learning architectures nowadays. However, a technique needs to be applied in the implementation to improve the time computing for the algorithm: Stochastic Gradient Descent.

This will be soon discussed in another article.
